# 7 循环网络
## 7.1 Word2Vec
```向量空间模型（Vector Space Model)``` 
能学习到单词向量的相似性  （男 -> 女）  // （爸 -> 妈)
### SKip-Gram
从原句推测目标字词
序列 ··· a · b · c ···  (b -> a), (c -> a)
### CBOW(Continuous Bag of Words)
从目标字词推出原句


```预测模型 Neural Probabilistic Language Model 通常用最大似然```

```

data_index = 0
def generate_batch(batch_size, num_skips, skip_window):
	global data_index
	assert batch_size % num_skips == 0          # num_skips 为网络的数据单位
	assert num_skips <= 2 * skip_window         # num_skips < 样本集合
	batch = np.ndarray(shape=(batch_size), dtype=np.int32)
	labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
	span = 2 * skip_window + 1 					# 单词总数
	buffer = collections.deque(maxlen=sapn)  	#bidirection queue

	for _ in range(span):
		buffer.append(data[data_index])
		data_index = (data_index + 1) % len(data)   # data_index+1 < len(data) data_index+=1
	for i in range(batch_size // num_skips):
		target = skip_window						# 目标单词
		targets_to_avoid = [skip_window]
		for j in range(num_skips):					# 随机生成num_skips个(目标单词, 语境词)对
			while target in targets_to_avoid:
				target = random.randint(0, span-1)
			targets_to_avoid.append(target)
			batch[i * num_skips + j] = buffer[skip_window]
			labels[i * num_skips + j, 0] = buffer[target]
		buffer.append(data[data_index])
		data_index = (data_index + 1) % len(data)
	return batch, labels	

def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):
	assert low_dim_embs: shape[0] >= len(labels), 'More labels than embeddings'
	plt.figure(figsize=(18, 18))
	for i, label in enumerate(labels):
		x, y = low_dim_embs[i,:]
		plt.scatter(x, y)
		plt.annotate(label, xy=(x, y), 
			xytext=(5,2),
			textcoords='offset points',
			ha='right',
			va='bottom')

from sklearn.manifold import Tsne 
tsne = Tsne(perplexity=30, n_components=2, init='pca', n_iter=5000)
low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])
labels = [reverse_dictionary[i] for i in range(plot_only)]
plot_with_labels(low_dim_embs, labels)
```
## 7.2 LSTM

```
class PTBModule(object):
	def __init__(self, is_training, config, input_):
		self._input = input_

		batch_size = input_.batch_size
		num_steps = input_.num_steps
		size = config.hidden_size
		vocab_size = config.vocab_size

		def lstm_cell():
			return tf.contrib.nn.LSTMCell(size, forget_bias=0., state_is_tuple=Ture)
		attn_cell = lstm_cell
		if is_training and config.keep_prob < 1:
			def attn_cell():
				return tf.contrib.rnn.DropoutWrapper(lstm_cell(), 
													output_keep_prob=config.keep_prob)
			cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)],
												state_is_tuple=True)

		self._initial_state = cell.zero_state(batch_size, tf.float32)

		with tf.device('/cpu:0'):
			embedding = tf.get_variable('embedding',	
										[vocab_size, size],
										dtype=tf.float32)
			inputs = tf.nn.embedding_lookup(embedding, input_.input_data)
		if is_training and config.keep_prob < 1:
			inputs = tf.nn.dropout(inputs, config.keep_prob)

		outputs = []
		state = self._initial_state

		with tf.variable_scope('RNN'):
			for time_step in range(num_steps):
				if time_step > 0:
					tf.get_variable_scope().reuse_variables()
				(cell_output, state) = cell(inputs[:, time_step, :], state)
				outputs.append(cell_output)

		output = tf.reshape(tf.concat(outputs, 1), [-1, size])
		softmax_w = tf.get_variable('softmax_w', 
									[size, vocab_size], 
									dtype=tf.float32)
		softmax_b = tf.get_variable('softmax_b', 
									[vocab_size], 
									dtype=tf.float32)
		logits = tf.matmul(output, softmax_w) + softmax_b
		loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
			[logits],
			[tf.reshape(input_.targets, [-1])],
			[tf.ones([batch_size * num_steps], dtype=tf.float32)])
		self.cost_ = cost = tf.reduce_sum(loss) / batch_size
		self._final_state = state

		if not is_training:
			return 

		self._lr = tf.Variable(0., trainable=False)
		tvars = tf.trainable_variables()
		grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
										config.max_grad_norm)
		optimizer = tf.train.GradientDescentOptimizer(self._lr)
		self._train_op = optimizer.apply_gradients(zip(grads, tvars),
			global_step=tf.contrib.framework.get_or_create_global_step())

		self._new_lr = tf.placeholder(tf.float32,
									shape=[], name='new_learning_rate')
		self._lr_update = tf.assign(self._lr, self._new_lr)

		def assign_lr(self, session, lr_value):
			session.run(self._lr_update, feed_dict={self._new_lr: lr_value})

		@property
		def input(self):
			return self._input
		

def run_epoch(session, model, eval_op=None, verbose=False):
	start_time = time.time()
	costs = 0.0 
	iters = 0
	state = session.run(model.initial_state)

	fetches = {'cost': model.cost, 'final_state': model.final_state}
	if eval_op is not None:
		fetches['eval_op'] = eval_op

	for step in range(model.input.epoch_size):
		feed_dict = {}
		for i, (c, h) in enumerate(model.initial_state):
			feed_dict[c] = state[i].c
			feed_dict[h] = state[i].h

		vals = session.run(fetches, feed_dict)
		cost = vals['cost']
		state = vals['final_state']

		costs += cost
		iters += model.input.num_steps
	return np.exp(costs / iters)


with tf.Graph.as_default():
	initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)

	with tf.name_scope('Train'):
		train_input = PTBInput(config=config, 
							data=train_data,
							name='TrainInput',)
		with tf.variable_scope('Model', reuse=None, initializer=initializer):
			m = PTBModule(is_training=True,
			 			config=config, 
			 			input_=train_input)



sv = tf.train.Supervisor()
with sv.managed_session() as sess:
	for i in range(config.max_max_epoch):
		lr_decay = config.lr_decay ** max(i+1 - config.max_epoch, 0.0)
		m.assign_lr(sess, config.learning_rate * lr_decay)


```
## 7.3 Bidirectional LSTM


```
def BiRNN(x, weights,biases):
	x = tf.transpose(x, [1, 0, 2])
	x = tf.reshape(x, [-1, n_input])
	x = tf.split(x, n_steps)

	lstm_fw_cell = tf.contrib.rnn.LSTMCell(n_hidden, forget_bias=1.0)
	lstm_bw_cell = tf.contrib.rnn.LSTMCell(n_hidden, forget_bias=1.)

	outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)
	return tf.matmul(outputs[-1], weights) + biases

pred = BiRNN(x, weights, biases)

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

cost_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

init = tf.global_variables_initializer()	

with tf.Session() as sess:
	sess.run(init)
	step = 1
	while step * batch_size < max_samples:
		batch_x = batch_x.reshape((batch_size, n_steps, n_input))
		sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
		if step % displacy_step == 0:
			acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y})
			step += 1


	test_len = 10000
	test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))
	test_label = mnist.test.labels[:test_len]

```
